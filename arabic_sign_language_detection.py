# -*- coding: utf-8 -*-
"""Arabic_Sign_Language_Detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12xpvIP5iVuB4Q6u65cw12SxuFb9yl8Gn
"""

!pip install tensorflow keras

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/

!kaggle datasets download -d muhammadalbrham/rgb-arabic-alphabets-sign-language-dataset

import zipfile
zip_ref = zipfile.ZipFile('/content/rgb-arabic-alphabets-sign-language-dataset.zip', 'r')
zip_ref.extractall('/content')
zip_ref.close()

import os
import random
import shutil
import zipfile
import numpy as np
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.image import load_img, img_to_array
from keras.utils import to_categorical

dataset_path = "/content/RGB ArSL dataset"

def check_dataset_contents(directory):
    print(f"Checking contents of the directory: {directory}")

    if not os.path.exists(directory):
        print("Directory does not exist.")
        return

    contents = os.listdir(directory)

    if not contents:
        print("The directory is empty.")
        return

    print("Contents of the directory:")
    for item in contents:
        item_path = os.path.join(directory, item)
        if os.path.isdir(item_path):
            print(f"Directory: {item}")
        else:
            print(f"File: {item}")

check_dataset_contents(dataset_path)

def load_and_preprocess_images(directory):
    images = []
    labels = []

    for category in os.listdir(directory):
        category_path = os.path.join(directory, category)
        if os.path.isdir(category_path):
            for img_name in os.listdir(category_path):
                img_path = os.path.join(category_path, img_name)
                if img_name.lower().endswith(('.jpg', '.jpeg', '.png', '.gif', '.bmp')):
                    try:
                        img = load_img(img_path, target_size=(128, 128))
                        img_array = img_to_array(img) / 255.0
                        images.append(img_array)
                        labels.append(category)
                    except Exception as e:
                        print(f"Error loading image {img_path}: {e}")
    return np.array(images), np.array(labels)

from PIL import ImageFile
ImageFile.LOAD_TRUNCATED_IMAGES = True

import os
from PIL import Image

def verify_images(folder_path):
    for root, dirs, files in os.walk(folder_path):
        for file in files:
            try:
                img_path = os.path.join(root, file)
                img = Image.open(img_path)  # Attempt to open the image
                img.verify()                # Verify the integrity of the image
            except (IOError, SyntaxError) as e:
                print(f"Corrupted image deleted: {img_path}")
                os.remove(img_path)  # Delete the corrupted image

# Use the function to check for corrupted images in the dataset folder
verify_images('/content/RGB ArSL dataset')

images, labels = load_and_preprocess_images(dataset_path)

from sklearn.preprocessing import LabelEncoder
label_encoder = LabelEncoder()
encoded_label = label_encoder.fit_transform(labels)
categorical_labels = to_categorical(encoded_label)
if len(images) > 0:
    print(f"Loaded {images.shape[0]} images with shape {images.shape[1:]} and labels: {np.unique(labels)}")
else:
    print("No images found. Please check the dataset path.")

X_train, X_test, y_train, y_test = train_test_split(images, categorical_labels, test_size=0.2, random_state=42)
X_test,X_val,y_test,y_val=train_test_split(X_test,y_test,test_size=0.5, random_state=42)
X_train = np.array(X_train)
X_val = np.array(X_val)
X_test = np.array(X_test)
y_train = np.array(y_train)
y_val = np.array(y_val)
y_test = np.array(y_test)
print(f'X_train shape is {X_train.shape}')
print(f'X_val shape is {X_val.shape}')
print(f'X_test shape is {X_test.shape}')
print(f'y_train shape is {y_train.shape}')
print(f'y_val shape is {y_val.shape}')
print(f'y_test shape is {y_test.shape}')

import tensorflow as tf
from tensorflow.keras.applications import MobileNet
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten, Dropout
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.optimizers import Adam
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, classification_report

input_shape = (128, 128, 3)
num_classes = 31

base_model = MobileNet(weights='imagenet', include_top=False, input_shape=input_shape)

model = Sequential([
    base_model,
    Flatten(),
    Dense(128, activation='relu'),
    Dropout(0.5),
    Dense(64, activation='relu'),
    Dense(32, activation='relu'),
    Dense(num_classes, activation='softmax')
])

optimizer = Adam(learning_rate=0.001)
model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])

early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)


model.build(input_shape=(None, 128, 128, 3))
print(model.summary())

history = model.fit(
    X_train,
    y_train,
    batch_size=32,
    epochs=35,
    validation_data=(X_val, y_val),
    callbacks=[early_stopping],
    verbose=1
)

test_loss, test_accuracy = model.evaluate(X_test, y_test)
print(f'Test Accuracy: {test_accuracy * 100:.2f}%')

test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=2)
print(f"Test loss: {test_loss}")
print(f"Test_Accuracy:{test_accuracy}")


import matplotlib.pyplot as plt

# Training plot and validation accuracy values
plt.figure(figsize=(12, 6))
plt.subplot( 1, 2, 1)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Val Accuracy')
plt.title ('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(loc='upper left')

#plot training  and loss values
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Val Loss')
plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(loc='upper left')

plt.show()

from google.colab import files
model.save('arabic_sign_language_model.h5')
files.download('arabic_sign_language_model.h5')